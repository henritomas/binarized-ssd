{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity test if larq works as intended\n",
    "\n",
    "import tensorflow as tf\n",
    "import larq as lq\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, MaxPooling2D, BatchNormalization, Activation, Flatten\n",
    "from larq.layers import QuantDense, QuantConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quantized layers except the first will use the same options\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "          kernel_quantizer=\"ste_sign\",\n",
    "          kernel_constraint=\"weight_clip\")\n",
    "\n",
    "input_shape = (train_images.shape[1:])\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # In the first layer we only quantize the weights and not the input\n",
    "    x = QuantConv2D(32, (3,3),\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False)(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = BatchNormalization(scale=False)(x)\n",
    "    \n",
    "    x = QuantConv2D(64, (3,3),\n",
    "                    use_bias=False,\n",
    "                    **kwargs)(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = BatchNormalization(scale=False)(x)\n",
    "    \n",
    "    x = QuantConv2D(64, (3,3),\n",
    "                    use_bias=False,\n",
    "                    **kwargs)(x)\n",
    "    x = BatchNormalization(scale=False)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = QuantDense(64, use_bias=False, **kwargs)(x)\n",
    "    x = BatchNormalization(scale=False)(x)\n",
    "    \n",
    "    x = QuantDense(10, use_bias=False, **kwargs)(x)\n",
    "    x = BatchNormalization(scale=False)(x)\n",
    "    outputs = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/email_ni_henri/anaconda3/envs/tfkeras15/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "+model stats---------------------------------------------------------------------------------------------------+\n",
      "| Layer                  Input prec.               Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs  32-bit MACs |\n",
      "|                              (bit)                            x 1       x 1    (kB)                          |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| input_1                          -  ((None, 28, 28, 1),)        0         0       0           ?            ? |\n",
      "| quant_conv2d                     -      (-1, 26, 26, 32)      288         0    0.04           0       194688 |\n",
      "| max_pooling2d                    -      (-1, 13, 13, 32)        0         0       0           0            0 |\n",
      "| batch_normalization              -      (-1, 13, 13, 32)        0        64    0.25           0            0 |\n",
      "| quant_conv2d_1                   1      (-1, 11, 11, 64)    18432         0    2.25     2230272            0 |\n",
      "| max_pooling2d_1                  -        (-1, 5, 5, 64)        0         0       0           0            0 |\n",
      "| batch_normalization_1            -        (-1, 5, 5, 64)        0       128    0.50           0            0 |\n",
      "| quant_conv2d_2                   1        (-1, 3, 3, 64)    36864         0    4.50      331776            0 |\n",
      "| batch_normalization_2            -        (-1, 3, 3, 64)        0       128    0.50           0            0 |\n",
      "| flatten                          -             (-1, 576)        0         0       0           0            0 |\n",
      "| quant_dense                      1              (-1, 64)    36864         0    4.50       36864            0 |\n",
      "| batch_normalization_3            -              (-1, 64)        0       128    0.50           0            0 |\n",
      "| quant_dense_1                    1              (-1, 10)      640         0    0.08         640            0 |\n",
      "| batch_normalization_4            -              (-1, 10)        0        20    0.08           0            0 |\n",
      "| activation                       -              (-1, 10)        0         0       0           ?            ? |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                       93088       468   13.19     2599552       194688 |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "+model summary---------------------------------+\n",
      "| Total params                      93.6 k     |\n",
      "| Trainable params                  93.1 k     |\n",
      "| Non-trainable params              468        |\n",
      "| Model size                        13.19 KiB  |\n",
      "| Model size (8-bit FP weights)     11.82 KiB  |\n",
      "| Float-32 Equivalent               365.45 KiB |\n",
      "| Compression Ratio of Memory       0.04       |\n",
      "| Number of MACs                    2.79 M     |\n",
      "| Ratio of MACs that are binarized  0.9303     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/email_ni_henri/anaconda3/envs/tfkeras15/lib/python3.6/site-packages/larq/quantizers.py:74: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 0.6512 - acc: 0.9091 - val_loss: 0.4517 - val_acc: 0.9634\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.4720 - acc: 0.9631 - val_loss: 0.4579 - val_acc: 0.9678\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.4476 - acc: 0.9694 - val_loss: 0.4822 - val_acc: 0.9611\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.4401 - acc: 0.9715 - val_loss: 0.4748 - val_acc: 0.9643\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.4307 - acc: 0.9751 - val_loss: 0.4629 - val_acc: 0.9665\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.4253 - acc: 0.9763 - val_loss: 0.4102 - val_acc: 0.9769\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4102 - acc: 0.9769\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, \n",
    "          batch_size=64, \n",
    "          epochs=6,\n",
    "          validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 97.69 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy {test_acc * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf115",
   "language": "python",
   "name": "tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
